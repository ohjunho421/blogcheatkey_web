import re
import json
import logging
import time
import traceback
from urllib.parse import urlparse
from django.conf import settings
from konlpy.tag import Okt
from anthropic import Anthropic
from research.models import ResearchSource, StatisticData
from key_word.models import Keyword, Subtopic
from content.models import BlogContent, MorphemeAnalysis
from accounts.models import User
from .substitution_generator import SubstitutionGenerator

logger = logging.getLogger(__name__)


class ContentGenerator:
    """
    Claude APIë¥¼ ì‚¬ìš©í•œ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± ì„œë¹„ìŠ¤
    - ìƒì„±ê³¼ ë™ì‹œì— ìµœì í™” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì½˜í…ì¸  ìƒì„±
    """
    
    def __init__(self):
        self.anthropic_api_key = settings.ANTHROPIC_API_KEY
        self.model = "claude-3-7-sonnet-20250219"
        self.client = Anthropic(api_key=self.anthropic_api_key)
        self.okt = Okt()
        self.max_retries = 5
        self.retry_delay = 2
        self.substitution_generator = SubstitutionGenerator()
    
    def generate_content(self, keyword_id, user_id, target_audience=None, business_info=None, custom_morphemes=None, subtopics=None):
        """
        í‚¤ì›Œë“œ ê¸°ë°˜ ë¸”ë¡œê·¸ ì½˜í…ì¸  ìƒì„± (ìµœì í™” ì¡°ê±´ ì¶©ì¡±)
        
        Args:
            keyword_id (int): í‚¤ì›Œë“œ ID
            user_id (int): ì‚¬ìš©ì ID
            target_audience (dict): íƒ€ê²Ÿ ë…ì ì •ë³´
            business_info (dict): ì‚¬ì—…ì ì •ë³´
            custom_morphemes (list): ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ëª©ë¡
            subtopics (list): ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬ëœ ì†Œì œëª© ëª©ë¡ (ê¸°ë³¸ê°’ None)
            
        Returns:
            int: ìƒì„±ëœ BlogContent ê°ì²´ì˜ ID
        """
        for attempt in range(self.max_retries):
            try:
                # í‚¤ì›Œë“œ ë° ê´€ë ¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                keyword = Keyword.objects.get(id=keyword_id)
                user = User.objects.get(id=user_id)
                
                # ì „ë‹¬ë°›ì€ ì†Œì œëª© ì‚¬ìš©, ì—†ìœ¼ë©´ DBì—ì„œ ì¡°íšŒ
                if subtopics is None:
                    subtopics = list(keyword.subtopics.order_by('order').values_list('title', flat=True))
                
                # ì—°êµ¬ ìë£Œ ê°€ì ¸ì˜¤ê¸°
                news_sources = ResearchSource.objects.filter(keyword=keyword, source_type='news')
                academic_sources = ResearchSource.objects.filter(keyword=keyword, source_type='academic')
                general_sources = ResearchSource.objects.filter(keyword=keyword, source_type='general')
                statistics = StatisticData.objects.filter(source__keyword=keyword)
                
                # ê¸°ì¡´ "ìƒì„± ì¤‘..." ì½˜í…ì¸  ì°¾ê¸° (ì‹œê°„ ì œí•œ ì—†ìŒ)
                existing_content = BlogContent.objects.filter(
                    keyword=keyword, 
                    user=user, 
                    title__contains="(ìƒì„± ì¤‘...)"
                ).order_by('-created_at').first()
                
                # í˜•íƒœì†Œ ë¶„ì„
                morphemes = self.okt.morphs(keyword.keyword)
                
                # ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ì¶”ê°€
                if custom_morphemes:
                    morphemes.extend(custom_morphemes)
                    morphemes = list(set(morphemes))  # ì¤‘ë³µ ì œê±°
                
                # ë°ì´í„° êµ¬ì„±
                data = {
                    "keyword": keyword.keyword,
                    "subtopics": subtopics,  # ì—¬ê¸°ì„œ ì „ë‹¬ë°›ì€ ì†Œì œëª© ì‚¬ìš©
                    "target_audience": target_audience or {
                        "primary": keyword.main_intent,
                        "pain_points": keyword.pain_points
                    },
                    "business_info": business_info or {
                        "name": user.username,
                        "expertise": ""
                    },
                    "morphemes": morphemes,
                    "research_data": self._format_research_data(
                        news_sources, 
                        academic_sources, 
                        general_sources, 
                        statistics
                    )
                }
                
                # ë¡œê¹… ì¶”ê°€ - API í˜¸ì¶œ ì‹œì‘ ì „
                logger.info(f"ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì‹œì‘: í‚¤ì›Œë“œ={keyword.keyword}, ì‚¬ìš©ì={user.username}")
                logger.info(f"ì½˜í…ì¸  ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” ì†Œì œëª©: {subtopics}")

                # ìµœì í™” ì¡°ê±´ì´ í¬í•¨ëœ ì½˜í…ì¸  ìƒì„± í”„ë¡¬í”„íŠ¸ 
                prompt = self._create_optimized_content_prompt(data)
                
                # API ìš”ì²­
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=8192,
                    temperature=0.7,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                
                # ë¡œê¹… ì¶”ê°€ - API í˜¸ì¶œ ì™„ë£Œ
                logger.info("ì½˜í…ì¸  ìƒì„± API í˜¸ì¶œ ì™„ë£Œ")
                
                content = response.content[0].text
                
                # ìµœì í™” ê²€ì¦
                verification_result = self._verify_content_optimization(content, keyword.keyword, morphemes)
                
                # ìµœì í™” ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ê²½ìš° ì¶”ê°€ ìµœì í™” ì‹œë„
                if not verification_result['is_fully_optimized']:
                    # ë¡œê¹… ì¶”ê°€ - ìµœì í™” ì‹œì‘
                    logger.info("ì½˜í…ì¸  ìµœì í™” ì‹œì‘: ë¯¸ë‹¬ ì¡°ê±´ ìˆìŒ")
                    logger.info(f"ê²€ì¦ ê²°ê³¼: ê¸€ììˆ˜={verification_result['char_count']}, ìœ íš¨={verification_result['is_valid_char_count']}, í˜•íƒœì†Œ ìœ íš¨={verification_result['is_valid_morphemes']}")
                    
                    # ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„± ë° API í˜¸ì¶œ
                    optimization_prompt = self._create_verification_optimization_prompt(
                        content, 
                        keyword.keyword, 
                        morphemes,
                        verification_result
                    )
                    
                    optimization_response = self.client.messages.create(
                        model=self.model,
                        max_tokens=8192,
                        temperature=0.5,
                        messages=[
                            {"role": "user", "content": optimization_prompt}
                        ]
                    )
                    
                    optimized_content = optimization_response.content[0].text
                    
                    # ìµœì¢… ê²€ì¦
                    final_verification = self._verify_content_optimization(optimized_content, keyword.keyword, morphemes)
                    
                    if final_verification['is_fully_optimized'] or final_verification['is_better_than'](verification_result):
                        content = optimized_content
                        logger.info("ìµœì í™”ëœ ì½˜í…ì¸  ì‚¬ìš©: ë” ë‚˜ì€ ê²°ê³¼")
                    else:
                        logger.info("ì›ë³¸ ì½˜í…ì¸  ì‚¬ìš©: ìµœì í™” ì‹œë„ í›„ì—ë„ ê°œì„ ë˜ì§€ ì•ŠìŒ")
                    
                    # ë¡œê¹… ì¶”ê°€ - ìµœì í™” ì™„ë£Œ
                    logger.info(f"ì½˜í…ì¸  ìµœì í™” ì™„ë£Œ: ê¸€ììˆ˜={final_verification['char_count']}, ìœ íš¨={final_verification['is_valid_char_count']}, í˜•íƒœì†Œ ìœ íš¨={final_verification['is_valid_morphemes']}")
                
                # ì°¸ê³  ìë£Œ ì¶”ê°€
                content_with_references = self._add_references(content, data['research_data'])
                
                # ëª¨ë°”ì¼ ìµœì í™” í¬ë§· ìƒì„±
                mobile_formatted_content = self._format_for_mobile(content_with_references)
                
                # ì°¸ê³  ìë£Œ ëª©ë¡ ì¶”ì¶œ
                references = self._extract_references(content_with_references)
                
                # ì´ì „ 'ìƒì„± ì¤‘' ì½˜í…ì¸  ì‚­ì œ
                if existing_content:
                    # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ë„ ê°™ì´ ì‚­ì œë¨ (CASCADE)
                    existing_content.delete()
                
                # ì½˜í…ì¸  ì €ì¥
                blog_content = BlogContent.objects.create(
                    user=user,
                    keyword=keyword,
                    title=f"{keyword.keyword} ì™„ë²½ ê°€ì´ë“œ",  # ê¸°ë³¸ ì œëª©, ë‚˜ì¤‘ì— ë³€ê²½ ê°€ëŠ¥
                    content=content_with_references,
                    mobile_formatted_content=mobile_formatted_content,
                    references=references,  # ì°¸ê³ ìë£Œ ëª©ë¡ ì €ì¥
                    char_count=len(content.replace(" ", "")),
                    is_optimized=True
                )
                
                # ë¡œê¹… ì¶”ê°€ - í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘
                logger.info("í˜•íƒœì†Œ ë¶„ì„ ì‹œì‘")
                
                # í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ ì €ì¥
                morpheme_analysis = self.analyze_morphemes(content, keyword.keyword, custom_morphemes)
                for morpheme, info in morpheme_analysis.get('morpheme_analysis', {}).items():
                    if morpheme and len(morpheme) > 1:  # 1ê¸€ì ë¯¸ë§Œì€ ì €ì¥í•˜ì§€ ì•ŠìŒ
                        MorphemeAnalysis.objects.create(
                            content=blog_content,
                            morpheme=morpheme,
                            count=info.get('count', 0),
                            is_valid=info.get('is_valid', False)
                        )
                
                # ë¡œê¹… ì¶”ê°€ - ì½˜í…ì¸  ìƒì„± ì™„ë£Œ
                logger.info(f"ì½˜í…ì¸  ìƒì„± ì™„ë£Œ: ID={blog_content.id}")
                
                return blog_content.id
                    
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                logger.error(traceback.format_exc())
                
                if 'overloaded_error' in str(e) and attempt < self.max_retries - 1:
                    logger.warning(f"ì„œë²„ê°€ í˜¼ì¡í•©ë‹ˆë‹¤. {self.retry_delay}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... ({attempt + 1}/{self.max_retries})")
                    time.sleep(self.retry_delay)
                    continue
                raise e
        
        return None
                    
    def _verify_content_optimization(self, content, keyword, morphemes):
        """
        ì½˜í…ì¸ ê°€ ìµœì í™” ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²€ì¦
        
        Args:
            content (str): ê²€ì¦í•  ì½˜í…ì¸ 
            keyword (str): ì£¼ìš” í‚¤ì›Œë“œ
            morphemes (list): í˜•íƒœì†Œ ëª©ë¡
            
        Returns:
            dict: ê²€ì¦ ê²°ê³¼
        """
        # ì°¸ê³ ìë£Œ ë¶„ë¦¬ (ê²€ì¦ ëŒ€ìƒì—ì„œ ì œì™¸)
        content_without_refs = content
        if "## ì°¸ê³ ìë£Œ" in content:
            content_without_refs = content.split("## ì°¸ê³ ìë£Œ", 1)[0]
        
        # ê¸€ììˆ˜ ê²€ì¦
        char_count = len(content_without_refs.replace(" ", ""))
        is_valid_char_count = 1700 <= char_count <= 2000
        
        # í˜•íƒœì†Œ ë¶„ì„
        morpheme_analysis = self.analyze_morphemes(content_without_refs, keyword)
        is_valid_morphemes = morpheme_analysis.get('is_valid', False)
        
        # ì™„ì „ ìµœì í™” ì—¬ë¶€
        is_fully_optimized = is_valid_char_count and is_valid_morphemes
        
        result = {
            'is_fully_optimized': is_fully_optimized,
            'char_count': char_count,
            'is_valid_char_count': is_valid_char_count,
            'morpheme_analysis': morpheme_analysis,
            'is_valid_morphemes': is_valid_morphemes,
            'content_without_refs': content_without_refs
        }
        
        # ë¹„êµ í•¨ìˆ˜ ì¶”ê°€
        result['is_better_than'] = lambda other: self._is_optimization_better(result, other)
        
        return result
    
    def _is_optimization_better(self, new_result, old_result):
        """
        ìƒˆ ìµœì í™” ê²°ê³¼ê°€ ì´ì „ ê²°ê³¼ë³´ë‹¤ ë‚˜ì€ì§€ ë¹„êµ
        
        Args:
            new_result (dict): ìƒˆ ê²€ì¦ ê²°ê³¼
            old_result (dict): ì´ì „ ê²€ì¦ ê²°ê³¼
            
        Returns:
            bool: ë” ë‚˜ì€ ê²°ê³¼ì´ë©´ True
        """
        # ëª¨ë“  ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ ë¹„êµ
        if new_result['is_fully_optimized'] and not old_result['is_fully_optimized']:
            return True
            
        # ê¸€ììˆ˜ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ ë¹„êµ
        if new_result['is_valid_char_count'] and not old_result['is_valid_char_count']:
            return True
            
        # í˜•íƒœì†Œ ì¡°ê±´ ë§Œì¡± ì—¬ë¶€ ë¹„êµ
        if new_result['is_valid_morphemes'] and not old_result['is_valid_morphemes']:
            return True
        
        # ìœ íš¨í•œ í˜•íƒœì†Œ ê°œìˆ˜ ë¹„êµ
        new_valid_count = sum(1 for m, info in new_result['morpheme_analysis'].get('morpheme_analysis', {}).items() 
                             if info.get('is_valid', False))
        old_valid_count = sum(1 for m, info in old_result['morpheme_analysis'].get('morpheme_analysis', {}).items() 
                             if info.get('is_valid', False))
        
        if new_valid_count > old_valid_count:
            return True
            
        # ê¸€ììˆ˜ê°€ ëª©í‘œì— ë” ê°€ê¹Œìš´ì§€ í™•ì¸
        if not new_result['is_valid_char_count'] and not old_result['is_valid_char_count']:
            target_center = (1700 + 2000) / 2  # ëª©í‘œ ë²”ìœ„ì˜ ì¤‘ê°„ê°’
            new_distance = abs(new_result['char_count'] - target_center)
            old_distance = abs(old_result['char_count'] - target_center)
            
            if new_distance < old_distance:
                return True
                
        return False
    
    def _format_research_data(self, news_sources, academic_sources, general_sources, statistics):
        """
        ì—°êµ¬ ìë£Œ í¬ë§·íŒ…
        """
        research_data = {
            'news': [],
            'academic': [],
            'general': [],
            'statistics': []
        }
        
        # ë‰´ìŠ¤ ìë£Œ
        for source in news_sources:
            research_data['news'].append({
                'title': source.title,
                'url': source.url,
                'snippet': source.snippet,
                'date': source.published_date.isoformat() if source.published_date else '',
                'source': source.author
            })
        
        # í•™ìˆ  ìë£Œ
        for source in academic_sources:
            research_data['academic'].append({
                'title': source.title,
                'url': source.url,
                'snippet': source.snippet,
                'date': source.published_date.isoformat() if source.published_date else '',
                'source': source.author
            })
        
        # ì¼ë°˜ ìë£Œ
        for source in general_sources:
            research_data['general'].append({
                'title': source.title,
                'url': source.url,
                'snippet': source.snippet,
                'date': source.published_date.isoformat() if source.published_date else '',
                'source': source.author
            })
        
        # í†µê³„ ìë£Œ
        for stat in statistics:
            research_data['statistics'].append({
                'value': stat.value,
                'context': stat.context,
                'pattern_type': stat.pattern_type,
                'source_url': stat.source.url,
                'source_title': stat.source.title,
                'source': stat.source.author,
                'date': stat.source.published_date.isoformat() if source.published_date else ''
            })
        
        return research_data
    
    def _create_optimized_content_prompt(self, data):
        """
        ìµœì í™” ì¡°ê±´ì´ í¬í•¨ëœ ì½˜í…ì¸  ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            data (dict): ì½˜í…ì¸  ìƒì„± ë°ì´í„°
            
        Returns:
            str: ì½˜í…ì¸  ìƒì„± í”„ë¡¬í”„íŠ¸
        """
        keyword = data["keyword"]
        morphemes = data.get("morphemes", self.okt.morphs(keyword))
        
        # 2ê¸€ì ë¯¸ë§Œ í˜•íƒœì†Œ ì œì™¸ (ì˜ë¯¸ìˆëŠ” í˜•íƒœì†Œë§Œ ë‚¨ê¹€)
        morphemes = [m for m in morphemes if len(m) >= 2]
        
        # í‚¤ì›Œë“œ êµ¬ì„± ìš”ì†Œ ë¶„ì„ (ë³µí•© í‚¤ì›Œë“œì¸ ê²½ìš°)
        is_compound_keyword = ' ' in keyword
        keyword_parts = []
        if is_compound_keyword:
            keyword_parts = [part for part in keyword.split() if len(part) >= 2]
        
        # ë³µí•© í‚¤ì›Œë“œ ì²˜ë¦¬ ì§€ì¹¨ ìƒì„±
        keyword_instruction = ""
        if is_compound_keyword and keyword_parts:
            # ë³µí•© í‚¤ì›Œë“œ ê²½ìš°ì˜ íŠ¹ë³„ ì§€ì¹¨
            keyword_instruction = f"""
            ì´ ê¸€ì—ì„œëŠ” ë³µí•© í‚¤ì›Œë“œ '{keyword}'ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì´ í‚¤ì›Œë“œì™€ ê·¸ êµ¬ì„± ìš”ì†Œ({', '.join(keyword_parts)})ì˜ ì¶œí˜„ íšŸìˆ˜ë¥¼ ì •í™•íˆ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤.
            
            - '{keyword}' ì „ì²´ í‚¤ì›Œë“œëŠ” ì •í™•íˆ 17-20íšŒ ì‚¬ìš©í•˜ì„¸ìš”.
            - êµ¬ì„± ìš”ì†Œì¸ {', '.join(keyword_parts)} ê°ê°ë„ ì •í™•íˆ 17-20íšŒ ì‚¬ìš©ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
            - ì£¼ì˜: êµ¬ì„± ìš”ì†Œê°€ ë³µí•© í‚¤ì›Œë“œ ì•ˆì— ì´ë¯¸ í¬í•¨ë  ê²½ìš°ë¥¼ ê³ ë ¤í•˜ì—¬, ë³µí•© í‚¤ì›Œë“œê°€ ì•„ë‹Œ í˜•íƒœë¡œ êµ¬ì„± ìš”ì†Œë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ì•„ë˜ íŒ¨í„´ì„ ë”°ë¥´ì„¸ìš”:
            * ë³µí•© í‚¤ì›Œë“œ '{keyword}'ë¥¼ XíšŒ ì‚¬ìš©í–ˆë‹¤ë©´
            * ê° êµ¬ì„± ìš”ì†ŒëŠ” (17-X)íšŒì—ì„œ (20-X)íšŒë§Œí¼ ì¶”ê°€ë¡œ ë‹¨ë… ì‚¬ìš©í•˜ì„¸ìš”
            * ì˜ˆ: '{keyword}'ë¥¼ 18íšŒ ì‚¬ìš©í–ˆë‹¤ë©´, '{keyword_parts[0]}'ëŠ” -1~2íšŒ, '{keyword_parts[1] if len(keyword_parts) > 1 else keyword_parts[0]}'ëŠ” -1~2íšŒ ì¶”ê°€ ì‚¬ìš©
            
            ì´ë ‡ê²Œ í•˜ë©´ Ctrl+Fë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ê° ë‹¨ì–´ê°€ ì •í™•íˆ 17-20íšŒ ë‚˜ì˜¤ê²Œ ë©ë‹ˆë‹¤.
            """
        
        # ì•ˆì „í•œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        target_audience = data.get('target_audience', {})
        business_info = data.get('business_info', {})
        research_data = data.get('research_data', {})
        
        # ì—°êµ¬ ìë£Œ í¬ë§·íŒ… (ìµœëŒ€ 2ê°œì”©ë§Œ ì‚¬ìš©)
        research_text = ""
        if isinstance(research_data, dict):
            news = research_data.get('news', [])[:2]
            academic = research_data.get('academic', [])[:2]
            general = research_data.get('general', [])[:2]
            
            if news:
                research_text += "ğŸ“° ë‰´ìŠ¤ ìë£Œ:\n"
                for item in news:
                    research_text += f"- {item.get('title', '')}: {item.get('snippet', '')}\n"
            
            if academic:
                research_text += "\nğŸ“š í•™ìˆ  ìë£Œ:\n"
                for item in academic:
                    research_text += f"- {item.get('title', '')}: {item.get('snippet', '')}\n"
                    
            if general:
                research_text += "\nğŸ” ì¼ë°˜ ìë£Œ:\n"
                for item in general:
                    research_text += f"- {item.get('title', '')}: {item.get('snippet', '')}\n"

        statistics_text = ""
        if isinstance(research_data.get('statistics'), list):
            statistics_text = "\nğŸ’¡ í™œìš© ê°€ëŠ¥í•œ í†µê³„ ìë£Œ:\n"
            for stat in research_data['statistics']:
                statistics_text += f"- {stat['context']} (ì¶œì²˜: {stat['source_title']})\n"

        # ìµœì í™” ì¡°ê±´ ì„¹ì…˜
        optimization_requirements = f"""
        âš ï¸ ì¤‘ìš”: ë‹¤ìŒ ìµœì í™” ì¡°ê±´ì„ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•©ë‹ˆë‹¤.
        
        1. ê¸€ììˆ˜ ì¡°ê±´: ì •í™•íˆ 1700-2000ì (ê³µë°± ì œì™¸, ì°¸ê³ ìë£Œ ì„¹ì…˜ ì œì™¸)
        - ì™„ì„± í›„ Ctrl+Fë¡œ ê²€ìƒ‰í•˜ì—¬ ê¸€ììˆ˜ í™•ì¸
        - ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ê±°ë‚˜ í•„ìš”ì‹œ í™•ì¥í•˜ì—¬ ì´ ë²”ìœ„ì— ë§ì¶”ê¸°
        
        2. í‚¤ì›Œë“œ ë° í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´:
        {keyword_instruction if is_compound_keyword else f"   - ì£¼ í‚¤ì›Œë“œ '{keyword}': ì •í™•íˆ 17-20íšŒ ì‚¬ìš©"}
        - ê¸°íƒ€ í˜•íƒœì†Œ({', '.join([m for m in morphemes if m not in keyword_parts and m != keyword])}): ì •í™•íˆ 17-20íšŒ ì‚¬ìš©
        - ì¤‘ìš”: Ctrl+Fë¡œ ê²€ìƒ‰í–ˆì„ ë•Œ ëª¨ë“  í‚¤ì›Œë“œì™€ í˜•íƒœì†Œê°€ 17-20íšŒ ë²”ìœ„ ë‚´ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤!
        
        3. í‚¤ì›Œë“œ ìµœì í™” ë°©ë²•:
        - ì§€ì‹œì–´ í™œìš©: "{keyword}ëŠ”" â†’ "ì´ê²ƒì€"
        - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ìƒëµ
        - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: ê³¼ë‹¤ ì‚¬ìš©ëœ ë‹¨ì–´ë¥¼ ì ì ˆí•œ ë™ì˜ì–´ë¡œ ëŒ€ì²´
        
        âœ“ ìµœì¢… ê²€ì¦: ìƒì„± ì™„ë£Œ í›„ Ctrl+Fë¡œ ê²€ìƒ‰í•˜ì—¬ ëª¨ë“  í‚¤ì›Œë“œì™€ í˜•íƒœì†Œê°€ ì •í™•íˆ 17-20íšŒ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.
        """
        logger.info(f"í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬ë˜ëŠ” ì†Œì œëª©: {data.get('subtopics', [])}")


        prompt = f"""
        ë‹¤ìŒ ì¡°ê±´ë“¤ì„ ì¤€ìˆ˜í•˜ì—¬ ì „ë¬¸ì„±ê³¼ ì¹œê·¼í•¨ì´ ì¡°í™”ëœ, ì½ê¸° ì‰½ê³  ì‹¤ìš©ì ì¸ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

        {optimization_requirements}

        í•„ìˆ˜ í™œìš© ìë£Œ:
        {research_text}
        
        í†µê³„ ìë£Œ (ë°˜ë“œì‹œ 1ê°œ ì´ìƒ í™œìš©):
        {statistics_text}

        **ì¤‘ìš” ì°¸ê³ ìë£Œ ì¸ìš© ì§€ì¹¨:**
        1. ë³¸ë¬¸ì—ì„œ [1], [2]ì™€ ê°™ì€ ì¸ìš©ë²ˆí˜¸ í‘œì‹œëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
        2. ëŒ€ì‹  "í•œêµ­ì„ìœ ê³µì‚¬ì˜ ë³´ê³ ì„œì— ë”°ë¥´ë©´" ë˜ëŠ” "APIì˜ ì—°êµ¬ ê²°ê³¼ì— ì˜í•˜ë©´" ë“± ì¶œì²˜ ì´ë¦„ì„ ì§ì ‘ ì–¸ê¸‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì¸ìš©í•˜ì„¸ìš”.
        3. ì°¸ê³ ìë£Œì˜ ì¶œì²˜ëª…ê³¼ ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì–¸ê¸‰í•´ì£¼ì„¸ìš”. (ì˜ˆ: "í•œêµ­ì„ìœ ê³µì‚¬ì— ë”°ë¥´ë©´ êµ­ë‚´ ìë™ì°¨ìš© ìœ¤í™œìœ  ìˆ˜ìš”ëŠ”...")
        4. ë§í¬ëŠ” ê¸€ í•˜ë‹¨ì˜ ì°¸ê³ ìë£Œ ì„¹ì…˜ì— ìë™ìœ¼ë¡œ ì¶”ê°€ë˜ë¯€ë¡œ ë³¸ë¬¸ì— URLì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
        5. ê° ì†Œì œëª© ì„¹ì…˜ì—ì„œ ìµœì†Œ 1ê°œ ì´ìƒì˜ ê´€ë ¨ ì°¸ê³ ìë£Œë¥¼ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì—¬ ì¸ìš©í•˜ì„¸ìš”.

        1. ê¸€ì˜ êµ¬ì¡°ì™€ í˜•ì‹
        - ì „ì²´ êµ¬ì¡°: ì„œë¡ (20%) - ë³¸ë¡ (60%) - ê²°ë¡ (20%)
        - ê° ì†Œì œëª©ì€ ### ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
        - ì†Œì œëª© êµ¬ì„±:
        ### {data['subtopics'][0] if len(data['subtopics']) > 0 else 'ì†Œì œëª©1'}
        ### {data['subtopics'][1] if len(data['subtopics']) > 1 else 'ì†Œì œëª©2'}
        ### {data['subtopics'][2] if len(data['subtopics']) > 2 else 'ì†Œì œëª©3'}
        ### {data['subtopics'][3] if len(data['subtopics']) > 3 else 'ì†Œì œëª©4'}
        - ì „ì²´ ê¸¸ì´: 1700-2000ì (ê³µë°± ì œì™¸)

        2. [í•„ìˆ˜] ì„œë¡  ì‘ì„± ê°€ì´ë“œ
        ë°˜ë“œì‹œ ë‹¤ìŒ êµ¬ì¡°ë¡œ ì„œë¡ ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1) ë…ìì˜ ê³ ë¯¼/ë¬¸ì œ ê³µê° (ë°˜ë“œì‹œ ìµœì‹  í†µê³„ë‚˜ ì—°êµ¬ ê²°ê³¼ ì¸ìš©)
        - ìˆ˜ì§‘ëœ í†µê³„ìë£Œë‚˜ ì—°êµ¬ê²°ê³¼ë¥¼ í™œìš©í•˜ì—¬ ë¬¸ì œì˜ ì‹¬ê°ì„±ì´ë‚˜ ì¤‘ìš”ì„± ê°•ì¡°
        - "ìµœê·¼ í•œêµ­ì„ìœ ê³µì‚¬ì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´..." ë˜ëŠ” "ë¯¸êµ­ì„ìœ í˜‘íšŒì˜ í†µê³„ì— ì˜í•˜ë©´..."ê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‹œì‘
        - "{keyword}ì— ëŒ€í•´ ê³ ë¯¼ì´ ë§ìœ¼ì‹ ê°€ìš”?"
        - íƒ€ê²Ÿ ë…ìì˜ êµ¬ì²´ì ì¸ ì–´ë ¤ì›€ ì–¸ê¸‰: {', '.join(target_audience.get('pain_points', []))}
        
        2) ì „ë¬¸ê°€ë¡œì„œì˜ í•´ê²°ì±… ì œì‹œ
        - "ì´ëŸ° ë¬¸ì œëŠ” {keyword}ë§Œ ì˜ ì•Œê³ ìˆì–´ë„ í•´ê²°ë˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤"
        - "{business_info.get('name', '')}ê°€ {business_info.get('expertise', '')}ì„ ë°”íƒ•ìœ¼ë¡œ í•´ê²°í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤"
        
        3) ë…ì ê´€ì‹¬ ìœ ë„
        - "ì´ ê¸€ì—ì„œëŠ” êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤" í›„ ì†Œì œëª© ë¯¸ë¦¬ë³´ê¸°
        - "5ë¶„ë§Œ íˆ¬ìí•˜ì‹œë©´ {keyword}ì— ëŒ€í•œ ëª¨ë“  ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤"

        3. ê¸€ì“°ê¸° ìŠ¤íƒ€ì¼
        - ì „ë¬¸ê°€ì˜ ì§€ì‹ì„ ì‰½ê²Œ ì„¤ëª…í•˜ë“¯ì´ í¸ì•ˆí•œ í†¤ ìœ ì§€
        - ê° ë¬¸ë‹¨ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ìŒ ë¬¸ë‹¨ìœ¼ë¡œ ì—°ê²°
        - ìŠ¤í† ë¦¬í…”ë§ ìš”ì†Œ í™œìš©
        - ì‹¤ì œ ì‚¬ë¡€ë‚˜ ë¹„ìœ ë¥¼ í†µí•´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…

        4. í•µì‹¬ í‚¤ì›Œë“œ í™œìš©
        - ì£¼ í‚¤ì›Œë“œ: {keyword}
        - í˜•íƒœì†Œ: {', '.join(morphemes)}
        - ê° í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ 17-20íšŒ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©
            
        5. [í•„ìˆ˜] ì°¸ê³  ìë£Œ í™œìš©
        - ê° ì†Œì œëª© ì„¹ì…˜ë§ˆë‹¤ ìµœì†Œ 1ê°œ ì´ìƒì˜ ê´€ë ¨ í†µê³„/ì—°êµ¬ ìë£Œ ë°˜ë“œì‹œ ì¸ìš©
        - ì¸ìš©í•  ë•ŒëŠ” "~ì— ë”°ë¥´ë©´", "~ì˜ ì—°êµ¬ ê²°ê³¼", "~ì˜ í†µê³„ì— ì˜í•˜ë©´" ë“± ëª…í™•í•œ í‘œí˜„ ì‚¬ìš©
        - ëª¨ë“  í†µê³„ì™€ ìˆ˜ì¹˜ëŠ” ì¶œì²˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ (ì˜ˆ: "2024ë…„ í•œêµ­ì„ìœ ê³µì‚¬ì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´...")
        - ê°€ëŠ¥í•œ ìµœì‹  ìë£Œë¥¼ ìš°ì„ ì ìœ¼ë¡œ í™œìš©
        - í†µê³„ë‚˜ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•  ë•ŒëŠ” ê·¸ ì˜ë¯¸ë‚˜ ì‹œì‚¬ì ë„ í•¨ê»˜ ì„¤ëª…

        6. ë³¸ë¡  ì‘ì„± ê°€ì´ë“œ
        - ê° ì†Œì œëª©ë§ˆë‹¤ í•µì‹¬ ì£¼ì œ í•œ ì¤„ ìš”ì•½ìœ¼ë¡œ ì‹œì‘
        - ì´ë¡  â†’ ì‚¬ë¡€ â†’ ì‹¤ì²œ ë°©ë²• ìˆœìœ¼ë¡œ êµ¬ì„±
        - ì°¸ê³  ìë£Œì˜ í†µê³„ë‚˜ ì—°êµ¬ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì¸ìš©
        - ì „ë¬¸ì  ë‚´ìš©ë„ ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…
        - ê° ì„¹ì…˜ ëì—ì„œ ë‹¤ìŒ ì„¹ì…˜ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°

        7. ê²°ë¡  ì‘ì„± ê°€ì´ë“œ
        - ë³¸ë¡  ë‚´ìš© ìš”ì•½
        - ì‹¤ì²œ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„ ì œì‹œ
        - "{business_info.get('name', '')}ê°€ ë„ì™€ë“œë¦´ ìˆ˜ ìˆë‹¤"ëŠ” ë©”ì‹œì§€
        - ë…ìì™€ì˜ ìƒí˜¸ì‘ìš© ìœ ë„

        ìœ„ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ, íŠ¹íˆ íƒ€ê²Ÿ ë…ì({target_audience.get('primary', '')})ì˜ ì–´ë ¤ì›€ì„ í•´ê²°í•˜ëŠ” ë° ì´ˆì ì„ ë§ì¶”ì–´ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
        """
        return prompt
    
    def _create_verification_optimization_prompt(self, content, keyword, morphemes, verification_result):
        """
        ê²€ì¦ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì í™” í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            content (str): ìµœì í™”í•  ì½˜í…ì¸ 
            keyword (str): ì£¼ìš” í‚¤ì›Œë“œ
            morphemes (list): í˜•íƒœì†Œ ëª©ë¡
            verification_result (dict): ê²€ì¦ ê²°ê³¼
            
        Returns:
            str: ìµœì í™” í”„ë¡¬í”„íŠ¸
        """
        # ìµœì í™”ê°€ í•„ìš”í•œ í˜•íƒœì†Œ ëª©ë¡
        morpheme_issues = []
        morpheme_analysis = verification_result['morpheme_analysis']
        
        for morpheme, info in morpheme_analysis.get('morpheme_analysis', {}).items():
            if not info.get('is_valid', True):
                count = info.get('count', 0)
                if count < 17:
                    morpheme_issues.append(f"- '{morpheme}': í˜„ì¬ {count}íšŒ â†’ 17-20íšŒë¡œ ì¦ê°€ í•„ìš” (+{17-count}íšŒ)")
                elif count > 20:
                    morpheme_issues.append(f"- '{morpheme}': í˜„ì¬ {count}íšŒ â†’ 17-20íšŒë¡œ ê°ì†Œ í•„ìš” (-{count-20}íšŒ)")
        
        morpheme_issues_text = "\n".join(morpheme_issues)
        
        # ê¸€ììˆ˜ ì¡°ì • ì•ˆë‚´
        char_count = verification_result['char_count']
        char_count_guidance = ""
        
        if char_count < 1700:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í˜„ì¬ {char_count}ì â†’ 1700-2000ìë¡œ ì¦ê°€ í•„ìš” (ìµœì†Œ {1700-char_count}ì ì¶”ê°€)"
        elif char_count > 2000:
            char_count_guidance = f"ê¸€ììˆ˜ê°€ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. í˜„ì¬ {char_count}ì â†’ 1700-2000ìë¡œ ê°ì†Œ í•„ìš” (ìµœì†Œ {char_count-2000}ì ì œê±°)"
        else:
            char_count_guidance = f"ê¸€ììˆ˜ëŠ” ì ì • ë²”ìœ„ì…ë‹ˆë‹¤ (í˜„ì¬ {char_count}ì). í˜•íƒœì†Œ ì¡°ì • ê³¼ì •ì—ì„œ ìœ ì§€í•˜ì„¸ìš”."
        
        # ìµœì í™” ì „ëµ ì œì‹œ - ë™ì  ëŒ€ì²´ì–´ ìƒì„± í™œìš©
        optimization_strategies = self._generate_dynamic_optimization_strategies(keyword, morpheme_analysis.get('morpheme_analysis', {}))
        
        return f"""
        ë‹¤ìŒ ë¸”ë¡œê·¸ ì½˜í…ì¸ ë¥¼ ìµœì í™”í•´ì£¼ì„¸ìš”. ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ì¶©ì¡±í•˜ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”:
        
        ========== ìµœì í™” ëª©í‘œ ==========
        
        1. ê¸€ììˆ˜ ì¡°ê±´: 1700-2000ì (ê³µë°± ì œì™¸)
           {char_count_guidance}
        
        2. í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ì¡°ê±´: ê° í˜•íƒœì†Œ ì •í™•íˆ 17-20íšŒ ì‚¬ìš©
           ì¡°ì •ì´ í•„ìš”í•œ í˜•íƒœì†Œ:
           {morpheme_issues_text}
        
        ========== ìµœì í™” ì „ëµ ==========
        {optimization_strategies}
        
        ========== ì¤‘ìš” ì§€ì¹¨ ==========
        
        1. ì½˜í…ì¸ ì˜ í•µì‹¬ ë©”ì‹œì§€ì™€ ì „ë¬¸ì„±ì€ ìœ ì§€í•˜ì„¸ìš”.
        2. ëª¨ë“  ì†Œì œëª©ê³¼ ì£¼ìš” ì„¹ì…˜ì„ ìœ ì§€í•˜ì„¸ìš”.
        3. ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì²´ì™€ íë¦„ì„ ìœ ì§€í•˜ì„¸ìš”.
        4. ëª¨ë“  í†µê³„ ìë£Œ ì¸ìš©ê³¼ ì¶œì²˜ í‘œì‹œë¥¼ ìœ ì§€í•˜ì„¸ìš”.
        5. ì¡°ì • í›„ì—ëŠ” ë°˜ë“œì‹œ ê° í˜•íƒœì†Œê°€ 17-20íšŒ ë²”ìœ„ ë‚´ì—ì„œ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
        6. ê²°ê³¼ë¬¼ë§Œ ì œì‹œí•˜ê³  ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
        
        ========== ì›ë³¸ ì½˜í…ì¸  ==========
        {content}
        """
    
    def _generate_dynamic_optimization_strategies(self, keyword, morpheme_analysis):
        """
        ë™ì ìœ¼ë¡œ í‚¤ì›Œë“œì™€ í˜•íƒœì†Œì— ëŒ€í•œ ìµœì í™” ì „ëµ ìƒì„±
        
        Args:
            keyword (str): ì£¼ìš” í‚¤ì›Œë“œ
            morpheme_analysis (dict): í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼
            
        Returns:
            str: ìµœì í™” ì „ëµ í…ìŠ¤íŠ¸
        """
        # ê³¼ë‹¤/ë¶€ì¡± í˜•íƒœì†Œ ë¶„ë¥˜
        excess_morphemes = []
        lacking_morphemes = []
        
        for morpheme, info in morpheme_analysis.items():
            count = info.get('count', 0)
            if count > 20:
                excess_morphemes.append(morpheme)
            elif count < 17:
                lacking_morphemes.append(morpheme)
        
        # ê¸°ë³¸ ì „ëµ ì œì‹œ
        strategies = """
        1. ê³¼ë‹¤ ì‚¬ìš©ëœ í˜•íƒœì†Œ ê°ì†Œ ë°©ë²•:
           - ë™ì˜ì–´/ìœ ì‚¬ì–´ ëŒ€ì²´: ë°˜ë³µë˜ëŠ” ìš©ì–´ë¥¼ ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë°”ê¾¸ê¸°
           - ì§€ì‹œì–´ ì‚¬ìš©: "ì´ê²ƒ", "ì´", "ê·¸", "í•´ë‹¹" ë“±ì˜ ì§€ì‹œì–´ë¡œ ëŒ€ì²´
           - ìì—°ìŠ¤ëŸ¬ìš´ ìƒëµ: ë¬¸ë§¥ìƒ ì´í•´ ê°€ëŠ¥í•œ ê²½ìš° ê³¼ê°íˆ ìƒëµ
           - ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ë¬¸ì¥ ì¬êµ¬ì„±: ê°™ì€ ì˜ë¯¸ë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
        
        2. ë¶€ì¡±í•œ í˜•íƒœì†Œ ì¦ê°€ ë°©ë²•:
           - êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ì‚¬ë¡€ ì¶”ê°€: í•´ë‹¹ í˜•íƒœì†Œê°€ í¬í•¨ëœ ì˜ˆì‹œ ì¶”ê°€
           - ì„¤ëª… í™•ì¥: í•µì‹¬ ê°œë…ì— ëŒ€í•œ ì¶”ê°€ ì„¤ëª… ì œê³µ
           - ì‹¤ìš©ì ì¸ íŒì´ë‚˜ ì¡°ì–¸ ì¶”ê°€: í˜•íƒœì†Œê°€ í¬í•¨ëœ íŒ ì œì‹œ
           - ê¸°ì¡´ ë¬¸ì¥ ë¶„ë¦¬: í•œ ë¬¸ì¥ì„ ë‘ ê°œë¡œ ë‚˜ëˆ„ì–´ í˜•íƒœì†Œ ì‚¬ìš© ê¸°íšŒ ì¦ê°€
        """
        
        # êµ¬ì²´ì ì¸ ëŒ€ì²´ì–´ ì œì•ˆ
        substitution_text = "\n3. ìœ ìš©í•œ ëŒ€ì²´ì–´ ì˜ˆì‹œ:"
        
        # í‚¤ì›Œë“œ ëŒ€ì²´ì–´
        keyword_substitutions = self.substitution_generator.get_substitutions(keyword)
        if keyword_substitutions:
            substitution_text += f"\n   - '{keyword}' ëŒ€ì²´ì–´: {', '.join(keyword_substitutions[:5])}"
        
        # ê³¼ë‹¤ ì‚¬ìš©ëœ ê° í˜•íƒœì†Œì— ëŒ€í•œ ëŒ€ì²´ì–´
        for morpheme in excess_morphemes:
            if morpheme != keyword:  # í‚¤ì›Œë“œëŠ” ì´ë¯¸ ì²˜ë¦¬ë¨
                morpheme_substitutions = self.substitution_generator.get_substitutions(keyword, morpheme)
                if morpheme_substitutions:
                    substitution_text += f"\n   - '{morpheme}' ëŒ€ì²´ì–´: {', '.join(morpheme_substitutions[:5])}"
        
        return strategies + substitution_text
        
    def analyze_morphemes(self, text, keyword=None, custom_morphemes=None):
        """
        í˜•íƒœì†Œ ë¶„ì„ ë° ì¶œí˜„ íšŸìˆ˜ ê²€ì¦
        
        Args:
            text (str): ë¶„ì„í•  í…ìŠ¤íŠ¸
            keyword (str): ì£¼ìš” í‚¤ì›Œë“œ
            custom_morphemes (list): ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ
            
        Returns:
            dict: í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼
        """
        if not keyword:
            return {}

        # ì •í™•í•œ ì¹´ìš´íŒ…ì„ ìœ„í•œ ì „ì²˜ë¦¬
        text = re.sub(r'<[^>]+>', '', text)  # HTML íƒœê·¸ ì œê±°
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)  # íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬ (í•œê¸€ í¬í•¨)
        
        # í‚¤ì›Œë“œì™€ í˜•íƒœì†Œ ì¶œí˜„ íšŸìˆ˜ ê³„ì‚°
        keyword_count = self._count_exact_word(keyword, text)
        morphemes = self.okt.morphs(keyword)
        
        # ì‚¬ìš©ì ì§€ì • í˜•íƒœì†Œ ì¶”ê°€
        if custom_morphemes:
            morphemes.extend(custom_morphemes)
        morphemes = list(set(morphemes))  # ì¤‘ë³µ ì œê±°

        analysis = {
            "is_valid": True,
            "morpheme_analysis": {},
            "needs_optimization": False
        }

        # í‚¤ì›Œë“œ ë¶„ì„
        analysis["morpheme_analysis"][keyword] = {
            "count": keyword_count,
            "is_valid": 17 <= keyword_count <= 20,
            "status": "ì ì •" if 17 <= keyword_count <= 20 else "ê³¼ë‹¤" if keyword_count > 20 else "ë¶€ì¡±"
        }

        # í˜•íƒœì†Œ ë¶„ì„
        for morpheme in morphemes:
            # 2ê¸€ì ë¯¸ë§Œ í˜•íƒœì†ŒëŠ” ë¶„ì„ì—ì„œ ì œì™¸
            if len(morpheme) < 2:
                continue
                
            count = self._count_exact_word(morpheme, text)
            is_valid = 17 <= count <= 20
            
            if not is_valid:
                analysis["is_valid"] = False
                analysis["needs_optimization"] = True

            analysis["morpheme_analysis"][morpheme] = {
                "count": count,
                "is_valid": is_valid,
                "status": "ì ì •" if is_valid else "ê³¼ë‹¤" if count > 20 else "ë¶€ì¡±"
            }

        return analysis

    def _count_exact_word(self, word, text):
        """
        í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ë‹¨ì–´ì˜ ì •í™•í•œ ì¶œí˜„ íšŸìˆ˜ë¥¼ ê³„ì‚°
        
        Args:
            word (str): ì°¾ì„ ë‹¨ì–´
            text (str): ê²€ìƒ‰í•  í…ìŠ¤íŠ¸
            
        Returns:
            int: ë‹¨ì–´ì˜ ì¶œí˜„ íšŸìˆ˜
        """
        # í•œê¸€ì˜ ê²½ìš° ê²½ê³„ê°€ ëª…í™•í•˜ì§€ ì•Šì•„ ë‹¤ë¥¸ íŒ¨í„´ í•„ìš”
        if re.search(r'[ê°€-í£]', word):
            pattern = rf'(?<![ê°€-í£]){re.escape(word)}(?![ê°€-í£])'
        else:
            pattern = rf'\b{re.escape(word)}\b'
        
        return len(re.findall(pattern, text))
        
    def _add_references(self, content, research_data):
        """
        ì½˜í…ì¸ ì— ì°¸ê³ ìë£Œ ì„¹ì…˜ ì¶”ê°€
        
        Args:
            content (str): ì›ë³¸ ì½˜í…ì¸ 
            research_data (dict): ì—°êµ¬ ìë£Œ ë°ì´í„°
            
        Returns:
            str: ì°¸ê³ ìë£Œê°€ ì¶”ê°€ëœ ì½˜í…ì¸ 
        """
        # ì´ë¯¸ ì°¸ê³ ìë£Œ ì„¹ì…˜ì´ ìˆëŠ”ì§€ í™•ì¸
        if "## ì°¸ê³ ìë£Œ" in content:
            return content
        
        # ì¸ìš©ëœ ì°¸ê³ ìë£Œ ì¶”ì¶œ
        references = []
        
        # ë‰´ìŠ¤ ìë£Œ ì¤‘ ì¸ìš©ëœ ìë£Œ ì°¾ê¸°
        for source in research_data.get('news', []):
            if self._find_citation_in_content(content, source):
                references.append({
                    'title': source.get('title', ''),
                    'url': source.get('url', ''),
                    'source': source.get('source', '')
                })
        
        # í•™ìˆ  ìë£Œ ì¤‘ ì¸ìš©ëœ ìë£Œ ì°¾ê¸°
        for source in research_data.get('academic', []):
            if self._find_citation_in_content(content, source):
                references.append({
                    'title': source.get('title', ''),
                    'url': source.get('url', ''),
                    'source': source.get('source', '')
                })
        
        # ì¼ë°˜ ìë£Œ ì¤‘ ì¸ìš©ëœ ìë£Œ ì°¾ê¸°
        for source in research_data.get('general', []):
            if self._find_citation_in_content(content, source):
                references.append({
                    'title': source.get('title', ''),
                    'url': source.get('url', ''),
                    'source': source.get('source', '')
                })
        
        # í†µê³„ ìë£Œì˜ ì¶œì²˜ ì¶”ê°€
        for stat in research_data.get('statistics', []):
            source_url = stat.get('source_url', '')
            source_title = stat.get('source_title', '')
            
            # ì´ë¯¸ ì¶”ê°€ëœ ì¶œì²˜ëŠ” ê±´ë„ˆë›°ê¸°
            if any(ref.get('url') == source_url for ref in references):
                continue
                
            if source_url and source_title and self._find_citation_in_content(content, {'title': source_title, 'snippet': stat.get('context', '')}):
                references.append({
                    'title': source_title,
                    'url': source_url,
                    'source': stat.get('source', '')
                })
        
        # ì°¸ê³ ìë£Œê°€ ì—†ìœ¼ë©´ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if not references:
            return content
        
        # ì°¸ê³ ìë£Œ ì„¹ì…˜ ì¶”ê°€
        reference_section = "\n\n## ì°¸ê³ ìë£Œ\n"
        
        for i, ref in enumerate(references, 1):
            title = ref.get('title', 'ì œëª© ì—†ìŒ')
            url = ref.get('url', '#')
            source = ref.get('source', '')
            
            # ì¶œì²˜ ì •ë³´ í¬í•¨
            if source:
                reference_section += f"{i}. [{title}]({url}) - {source}\n"
            else:
                reference_section += f"{i}. [{title}]({url})\n"
        
        return content + reference_section

    def _extract_references(self, content):
        """
        ì½˜í…ì¸ ì—ì„œ ì°¸ê³ ìë£Œ ë§í¬ ì¶”ì¶œ
        
        Args:
            content (str): ì½˜í…ì¸ 
            
        Returns:
            list: ì°¸ê³ ìë£Œ ëª©ë¡
        """
        references = []
        
        # ì°¸ê³ ìë£Œ ì„¹ì…˜ ì°¾ê¸°
        if "## ì°¸ê³ ìë£Œ" in content:
            refs_section = content.split("## ì°¸ê³ ìë£Œ", 1)[1]
            
            # ë§ˆí¬ë‹¤ìš´ ë§í¬ ì¶”ì¶œ íŒ¨í„´
            link_pattern = r'\[(.*?)\]\((.*?)\)'
            matches = re.findall(link_pattern, refs_section)
            
            for title, url in matches:
                # ì¶œì²˜ ì •ë³´ ì¶”ì¶œ (ìˆëŠ” ê²½ìš°)
                source = ""
                if " - " in title:
                    title_parts = title.split(" - ", 1)
                    title = title_parts[0]
                    source = title_parts[1]
                
                references.append({
                    'title': title.strip(),
                    'url': url.strip(),
                    'source': source.strip()
                })
        
        return references

    def _format_for_mobile(self, content):
        """
        ëª¨ë°”ì¼ í™”ë©´ì— ìµœì í™”ëœ í¬ë§·ìœ¼ë¡œ ë³€í™˜
        í•œê¸€ ê¸°ì¤€ 23ì ë‚´ì™¸ë¡œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬
        """
        # ì œëª©, ì†Œì œëª© ì²˜ë¦¬ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ìœ ì§€)
        lines = content.split('\n')
        formatted_lines = []
        
        for line in lines:
            # ë§ˆí¬ë‹¤ìš´ ì œëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            if line.strip().startswith('#'):
                formatted_lines.append(line)
                continue
                
            # ë¹ˆ ì¤„ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            if not line.strip():
                formatted_lines.append(line)
                continue
                
            # ëª©ë¡(ë¦¬ìŠ¤íŠ¸) í•­ëª©ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            if line.strip().startswith(('- ', '* ', '1. ', '2. ', '3. ')):
                formatted_lines.append(line)
                continue
            
            # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” 23ì ë‚´ì™¸ë¡œ ë¶„ë¦¬
            words = line.split()
            current_line = ""
            
            for word in words:
                # í˜„ì¬ ì¤„ + ìƒˆ ë‹¨ì–´ê°€ 23ìë¥¼ ì´ˆê³¼í•˜ë©´ ìƒˆ ì¤„ë¡œ
                if len((current_line + " " + word).replace(" ", "")) > 23 and current_line:
                    formatted_lines.append(current_line)
                    current_line = word
                else:
                    current_line = (current_line + " " + word).strip()
            
            # ë§ˆì§€ë§‰ ì¤„ ì¶”ê°€
            if current_line:
                formatted_lines.append(current_line)
        
        return '\n'.join(formatted_lines)
    
    def _find_citation_in_content(self, content, source_info):
        """
        ë³¸ë¬¸ì—ì„œ ì¸ìš© ì—¬ë¶€ í™•ì¸
        
        Args:
            content (str): ë³¸ë¬¸ ì½˜í…ì¸ 
            source_info (dict): ì¶œì²˜ ì •ë³´
            
        Returns:
            bool: ì¸ìš© ì—¬ë¶€
        """
        content_lower = content.lower()
        title = source_info.get('title', '').lower()
        author = source_info.get('source', '').lower()
        snippet = source_info.get('snippet', '').lower()
        
        # ì¶œì²˜ ì´ë¦„ í™•ì¸
        source_name = None
        if author and len(author) > 2:
            source_name = author
        elif title:
            # ì œëª©ì—ì„œ ê°€ëŠ¥í•œ ì¶œì²˜ ì´ë¦„ ì¶”ì¶œ (ì²« ëª‡ ë‹¨ì–´)
            title_words = title.split()
            if len(title_words) >= 2:
                source_name = ' '.join(title_words[:2])
        
        # ì¶œì²˜ ì´ë¦„ì´ ë³¸ë¬¸ì—ì„œ ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if source_name and source_name in content_lower:
            return True
        
        # ì¸ìš© íŒ¨í„´ í™•ì¸
        citation_patterns = [
            "ì— ë”°ë¥´ë©´",
            "ì˜ ì—°êµ¬ì— ë”°ë¥´ë©´",
            "ì˜ ì¡°ì‚¬ì— ë”°ë¥´ë©´",
            "ì˜ ë³´ê³ ì„œì— ë”°ë¥´ë©´",
            "ì—ì„œ ë°œí‘œí•œ",
            "ì˜ ë°œí‘œì— ë”°ë¥´ë©´",
            "ì—ì„œ ì¡°ì‚¬í•œ",
            "ì˜ í†µê³„ì— ì˜í•˜ë©´",
            "ì—ì„œ ì œì‹œí•œ",
            "ì˜ ìë£Œì— ë”°ë¥´ë©´"
        ]
        
        # 1. ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        numbers = re.findall(r'\d+(?:\.\d+)?%?', snippet)
        key_phrases = re.findall(r'[^\s,]+\s[^\s,]+\s[^\s,]+', snippet)
        
        # 2. ì¸ìš© íŒ¨í„´ê³¼ í•¨ê»˜ í•µì‹¬ ì •ë³´ê°€ ì‚¬ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for pattern in citation_patterns:
            for number in numbers:
                if f"{pattern} {number}" in content_lower:
                    return True
            for phrase in key_phrases:
                if f"{pattern} {phrase}" in content_lower:
                    return True
        
        # 3. ì œëª©ì´ë‚˜ ìŠ¤ë‹ˆí«ì˜ í•µì‹¬ ë‚´ìš©ì´ ë³¸ë¬¸ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        # ìµœì†Œ 3ë‹¨ì–´ ì´ìƒì˜ ì—°ì†ëœ êµ¬ë¬¸ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
        title_phrases = re.findall(r'[^\s,]+\s[^\s,]+\s[^\s,]+', title)
        snippet_phrases = re.findall(r'[^\s,]+\s[^\s,]+\s[^\s,]+', snippet)
        
        for phrase in title_phrases + snippet_phrases:
            if phrase in content_lower:
                return True
        
        return False